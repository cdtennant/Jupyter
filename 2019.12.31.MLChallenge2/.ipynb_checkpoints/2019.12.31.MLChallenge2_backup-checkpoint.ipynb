{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JLab ML Challenge 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import gzip\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "from keras.models import load_model\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Reshape, Flatten, Input, Lambda\n",
    "from keras.optimizers import SGD, Adamax, Adadelta\n",
    "from keras.initializers import glorot_normal\n",
    "from keras.callbacks import Callback, TensorBoard\n",
    "from keras.utils.training_utils import multi_gpu_model\n",
    "import keras.backend as K\n",
    "import keras.losses\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "EPOCHS = 1000\n",
    "BS     = 5000\n",
    "GPUS   = 0\n",
    "\n",
    "# Open labels files so we can get number of samples and pass the\n",
    "# data frames to the generators later\n",
    "traindf = pd.read_csv('TRAIN/track_parms.csv')\n",
    "validdf = pd.read_csv('VALIDATION/track_parms.csv')\n",
    "STEP_SIZE_TRAIN = len(traindf)/BS\n",
    "STEP_SIZE_VALID = len(validdf)/BS\n",
    "\n",
    "#-----------------------------------------------------\n",
    "# generate_arrays_from_file\n",
    "#-----------------------------------------------------\n",
    "# Create generator to read in images and labels\n",
    "# (used for both training and validation samples)\n",
    "def generate_arrays_from_file( path, labelsdf ):\n",
    "\n",
    "        images_path = path+'/images.raw.gz'\n",
    "        print( 'generator created for: ' + images_path)\n",
    "\n",
    "        batch_input       = []\n",
    "        batch_labels_tanl = []\n",
    "        batch_labels_z    = []\n",
    "        idx = 0\n",
    "        ibatch = 0\n",
    "        while True:  # loop forever, re-reading images from same file\n",
    "                with gzip.open(images_path) as f:\n",
    "                        while True: # loop over images in file\n",
    "\n",
    "                                # Read in one image\n",
    "                                bytes = f.read(width*height)\n",
    "                                if len(bytes) != (width*height): break # break into outer loop so we can re-open file\n",
    "                                data = np.frombuffer(bytes, dtype='B', count=width*height)\n",
    "                                pixels = np.reshape(data, [width, height, 1], order='F')\n",
    "                                pixels_norm = np.transpose(pixels.astype(np.float) / 255., axes=(1, 0, 2) )\n",
    "                                pixels_norm = 1.0 - pixels_norm  # Invert color (make black be no hit and white be tdrift=0\n",
    "\n",
    "                                # Read one label\n",
    "                                tanl = labelsdf.tanl[idx]\n",
    "                                z    = labelsdf.z[idx]\n",
    "                                idx += 1\n",
    "\n",
    "                                # Add to batch and check if it is time to yield\n",
    "                                batch_input.append( pixels_norm )\n",
    "                                batch_labels_tanl.append( tanl )\n",
    "                                batch_labels_z.append( z )\n",
    "                                if len(batch_input) == BS :\n",
    "                                        ibatch += 1\n",
    "\n",
    "                                        # Since we are training multiple loss functions we must\n",
    "                                        # pass the labels back as a dictionary whose keys match\n",
    "                                        # the layer their corresponding values are being applied\n",
    "                                        # to.\n",
    "                                        labels_dict = {\n",
    "                                                'tanl_output' :  np.array(batch_labels_tanl ),\n",
    "                                                'z_output'    :  np.array(batch_labels_z   ),\n",
    "                                        }\n",
    "\n",
    "                                        yield ( np.array(batch_input), labels_dict )\n",
    "                                        batch_input       = []\n",
    "                                        batch_labels_tanl = []\n",
    "                                        batch_labels_z    = []\n",
    "\n",
    "                        idx = 0\n",
    "                        f.close()\n",
    "\n",
    "#-----------------------------------------------------\n",
    "# MyWeightedAvg\n",
    "#\n",
    "# This is used by the final Lambda layer in each branch\n",
    "# of the network. It defines the formula for calculating\n",
    "# the weighted average of the inputs from the previous\n",
    "# layer.\n",
    "#-----------------------------------------------------\n",
    "def MyWeightedAvg(inputs, binsize, xmin):\n",
    "        ones = K.ones_like(inputs[0,:])                       # [1, 1, 1, 1....]   (size Nouts)\n",
    "        idx  = K.cumsum(ones)                                 # [1, 2, 3, 4....]   (size Nouts)\n",
    "        norm = K.sum(inputs, axis=1, keepdims=True)           # normalization of all outputs by batch. shape is 1D array of size batch (n.b. keepdims=True is critical!)\n",
    "        wsum = K.sum(idx*inputs, axis=1, keepdims=True)/norm  # array of size batch with weighted avg. of mean in units of bins (n.b. keepdims=True is critical!)\n",
    "        output = (binsize*(wsum-0.5)) + xmin                  # convert from bins to physical units (shape batch,1)\n",
    "\n",
    "        print('MyWeightedAvg:')\n",
    "        print('       binsize = %f' % binsize)\n",
    "        print('          xmin = %f' % xmin)\n",
    "        print('   input shape = %s' % str(inputs.shape))\n",
    "        print('  output shape = %s' % str(output.shape))\n",
    "\n",
    "        return output\n",
    "\n",
    "#-----------------------------------------------------\n",
    "# DefineCommonModel\n",
    "#-----------------------------------------------------\n",
    "def DefineCommonModel(inputs):\n",
    "        x = Flatten(name='top_layer1')(inputs)\n",
    "        x = Dense(int(Nouts*5), name='common_layer1', activation='linear', kernel_initializer=\"glorot_uniform\")(x)\n",
    "        #x = Dense(Nouts, name='common_layer2', activation='linear', kernel_initializer=\"glorot_uniform\")(x)\n",
    "        return x\n",
    "\n",
    "#-----------------------------------------------------\n",
    "# DefineTanlModel\n",
    "#-----------------------------------------------------\n",
    "def DefineTanlModel(inputs):\n",
    "        x = Dense(Nouts, name='tanl_output_dist', activation='relu', kernel_initializer=\"glorot_uniform\")(inputs)\n",
    "        x = Lambda(MyWeightedAvg, output_shape=(1,), name='tanl_output', arguments={'binsize':TANL_BINSIZE, 'xmin':TANLMIN})(x)\n",
    "        return x\n",
    "\n",
    "#-----------------------------------------------------\n",
    "# DefineZModel\n",
    "#-----------------------------------------------------\n",
    "def DefineZModel(inputs):\n",
    "        x = Dense(Nouts, name='z_hidden_layer1', activation='linear', kernel_initializer=\"glorot_uniform\")(inputs)\n",
    "        x = Dense(Nouts, name='z_output_dist', activation='relu', kernel_initializer=\"glorot_uniform\")(x)\n",
    "        x = Lambda(MyWeightedAvg, output_shape=(1,), name='z_output', arguments={'binsize':Z_BINSIZE, 'xmin':ZMIN})(x)\n",
    "        return x\n",
    "\n",
    "#-----------------------------------------------------\n",
    "# DefineModel\n",
    "#-----------------------------------------------------\n",
    "# This is used to define the model. It is only called if no model\n",
    "# file is found in the model_checkpoints directory.\n",
    "def DefineModel():\n",
    "\n",
    "        # If GPUS==0 this will force use of CPU, even if GPUs are present\n",
    "        # If GPUS>1 this will force the CPU to serve as orchestrator\n",
    "        # If GPUS==1 this will do nothing, allowing GPU to act as its own orchestrator\n",
    "        if GPUS!=1: tf.device('/cpu:0')\n",
    "\n",
    "        # Here we build the network model.\n",
    "        # This model is made of multiple parts. The first handles the\n",
    "        # inputs and identifies common features. The rest are branches with\n",
    "        # each determining an output parameter from those features.\n",
    "        inputs      = Input(shape=(height, width, 1), name='image_inputs')\n",
    "        commonmodel = DefineCommonModel(inputs)\n",
    "        tanlmodel   = DefineTanlModel( commonmodel )\n",
    "        zmodel      = DefineZModel( commonmodel )\n",
    "        model       = Model(inputs=inputs, outputs=[tanlmodel, zmodel])\n",
    "        #model       = Model(inputs=inputs, outputs=[tanlmodel])\n",
    "        model.summary()\n",
    "\n",
    "        # Here we specify a different loss function for every output branch.\n",
    "        # We also specify a weight for each branch. The weights allow us to \n",
    "        # specify that it is more important to minimize certain losses more\n",
    "        # than others.\n",
    "        sigma_tanl = 1.0  # estimated resolution \n",
    "        sigma_z    = 1.0  # estimated resolution\n",
    "        losses = {\n",
    "                'tanl_output'   :  'mean_squared_error',\n",
    "                'z_output'      :  'mean_squared_error',\n",
    "        }\n",
    "        lossWeights = {\n",
    "                'tanl_output'   :  1.0/(sigma_tanl*sigma_tanl),\n",
    "                'z_output'      :  1.0/(sigma_z*sigma_z),\n",
    "        }\n",
    "\n",
    "        # Compile the model, possibly using multiple GPUs\n",
    "        opt = Adadelta(clipnorm=1.0)\n",
    "        if GPUS<=1 :\n",
    "                final_model = model\n",
    "        else:\n",
    "                final_model = multi_gpu_model( model, gpus=GPUS )\n",
    "\n",
    "        final_model.compile(loss=losses, loss_weights=lossWeights, optimizer=opt, metrics=['mae', 'mse', 'accuracy'])\n",
    "\n",
    "        return final_model\n",
    "\n",
    "\n",
    "#===============================================================================\n",
    "\n",
    "\n",
    "# Here we want to check if a model has been saved due to previous training.\n",
    "# If so, then we read it in and continue training where it left off. Otherwise,\n",
    "# we define the model and start fresh. \n",
    "\n",
    "if not os.path.exists('model_checkpoints'): os.mkdir('model_checkpoints')\n",
    "\n",
    "# Look for most recent saved epoch\n",
    "epoch_loaded = 0\n",
    "for f in os.listdir('model_checkpoints'):\n",
    "        if f.startswith('model_epoch') and f.endswith('.h5'):\n",
    "                e = int(f[11:-3])\n",
    "                if e > epoch_loaded:\n",
    "                        epoch_loaded = e\n",
    "                        fname = 'model_checkpoints/model_epoch%03d.h5' % epoch_loaded\n",
    "\n",
    "if epoch_loaded > 0:\n",
    "        print('Loading model: ' + fname)\n",
    "        model = load_model( fname )\n",
    "else:\n",
    "        print('Unable to find saved model. Will start from scratch')\n",
    "        model = DefineModel()\n",
    "\n",
    "# Print summary of model\n",
    "model.summary()\n",
    "\n",
    "#===============================================================================\n",
    "\n",
    "# Create training and validation generators\n",
    "train_generator = generate_arrays_from_file('TRAIN', traindf)\n",
    "valid_generator = generate_arrays_from_file('VALIDATION', validdf)\n",
    "\n",
    "# Use tensorboard to log training. To view the training log with the\n",
    "# tensorboard gui you can run tensorboard to fire up a web server\n",
    "# so you can use your browser to view the results.\n",
    "#\n",
    "# Note: you may need to move the log file to your\n",
    "# local desktop and run tensorboard there.\n",
    "#\n",
    "#  tensorboard --logdir=./logs\n",
    "tensorboard=TensorBoard(log_dir='./logs', histogram_freq=0, batch_size=BS*GPUS, write_graph=True, write_grads=False, write_images=False, embeddings_freq=0, embeddings_layer_names=None, embeddings_metadata=None, embeddings_data=None, update_freq='epoch')\n",
    "\n",
    "#-----------------------------------------------------\n",
    "# class checkpointModel\n",
    "#-----------------------------------------------------\n",
    "# There is a bug in keras that causes an error when trying to save a model\n",
    "# trained on multiple GPUs. The work around is to save the original model\n",
    "# at the end of every epoch using a callback. See\n",
    "#    https://github.com/keras-team/kersas/issues/8694\n",
    "class checkpointModel(Callback):\n",
    "        def __init__(self, model):\n",
    "                self.model_to_save = model\n",
    "        def on_epoch_end(self, epoch, logs=None):\n",
    "                myepoch = epoch +1\n",
    "                fname = 'model_checkpoints/model_epoch%03d.h5' % myepoch\n",
    "                old_fname = 'model_checkpoints/model_epoch%03d.h5' % (myepoch-1)\n",
    "                if os.path.exists( old_fname ):\n",
    "                        print('removing old model: %s' % old_fname)\n",
    "                        os.remove( old_fname )\n",
    "                print('saving model: %s' % fname)\n",
    "                self.model_to_save.save(fname)\n",
    "cbk = checkpointModel( model )\n",
    "\n",
    "cbk.on_epoch_end(-1)\n",
    "\n",
    "# Fit the model\n",
    "history = model.fit_generator(\n",
    "  generator            = train_generator\n",
    "  ,steps_per_epoch     = STEP_SIZE_TRAIN\n",
    "  ,validation_data     = valid_generator\n",
    "  ,validation_steps    = STEP_SIZE_VALID\n",
    "  ,epochs              = EPOCHS\n",
    "  ,initial_epoch       = epoch_loaded\n",
    "  ,use_multiprocessing = False\n",
    "  ,callbacks=[tensorboard, cbk]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_10 (LSTM)               (None, None, 500)         1016000   \n",
      "_________________________________________________________________\n",
      "lstm_11 (LSTM)               (None, None, 500)         2002000   \n",
      "_________________________________________________________________\n",
      "lstm_12 (LSTM)               (None, None, 500)         2002000   \n",
      "_________________________________________________________________\n",
      "time_distributed_3 (TimeDist (None, None, 5)           2505      \n",
      "=================================================================\n",
      "Total params: 5,022,505\n",
      "Trainable params: 5,022,505\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import gzip\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Dense, TimeDistributed, Input, Lambda, LSTM\n",
    "\n",
    "model = Sequential()\n",
    "model.add( LSTM(units=500, return_sequences=True, input_shape=(None,7)) )\n",
    "model.add( LSTM(units=500, return_sequences=True) )\n",
    "model.add( LSTM(units=500, return_sequences=True) )\n",
    "model.add( TimeDistributed(Dense(units=5)) )\n",
    "\n",
    "model.compile(optimizer='adam', loss='mean_squared_error', metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 90009 (90009)\n",
      " Testing samples: 10002 (10002)\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "with open('MLchallenge2_training.csv') as csv_file:\n",
    "    csv_file.readline() # discard header line\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    \n",
    "    x_all = []\n",
    "    y_all = []\n",
    "    for row in csv_reader:\n",
    "        \n",
    "        # Copy each state vector (6 values) into individual list with a 7th value being the z of the next detector plane\n",
    "        statevectors = []\n",
    "        for i in range(0, 24):\n",
    "            idx = i*6\n",
    "            statevectors.append(row[idx:idx+6]+[row[idx+6+2]])\n",
    "\n",
    "        # Create 5-parameter labels for detector planes 7-24 and add them alomng with their corresponding features to the x_all and y_all lists\n",
    "        for i in range(7,24):\n",
    "            features = statevectors[0:i]\n",
    "            label = statevectors[i][0:2] + statevectors[i][3:6]\n",
    "            x_all.append(features)\n",
    "            y_all.append(label)\n",
    "            #print('FEATURES')\n",
    "            #print(features)\n",
    "            #print('LABEL')\n",
    "            #print(label)\n",
    "        \n",
    "        # Limit how many samples we use\n",
    "        if len(y_all) >=100000: break\n",
    "\n",
    "TRAIN_FRACTION = 0.90\n",
    "idx = int(len(x_all)*TRAIN_FRACTION)\n",
    "x_train = np.array(x_all[0:idx])\n",
    "y_train = np.array(y_all[0:idx])\n",
    "x_test  = np.array(x_all[idx:])\n",
    "y_test  = np.array(y_all[idx:])\n",
    "\n",
    "print('Training samples: ' + str(len(x_train)) + ' (' + str(len(y_train)) +')')\n",
    "print(' Testing samples: ' + str(len(x_test )) + ' (' + str(len(y_test )) +')')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to train with batch size>1 in order to be efficient. A batch, however, needs all members to\n",
    "# have the same number of time steps. It's OK for different batches to have a different number of time\n",
    "# steps, but within one batch, they must be the same.\n",
    "#\n",
    "# Here, we use global variables to keep track of the number of time steps the next batch should have\n",
    "# as well as the next index in the global training set for that particular number of steps. We need\n",
    "# the second index in case we have a different number of samples with say 17 time steps than we do with\n",
    "# 18 time steps.\n",
    "\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "def my_generator(x_samples, y_samples):\n",
    "    global BATCH_SIZE\n",
    "\n",
    "    BATCH_NSTEPS = 7\n",
    "    BATCH_STEP_INDEX = [0]*(24-7+1)\n",
    "\n",
    "    print('my_generator: ' + str(len(x_samples)) + ' samples')\n",
    "    \n",
    "    x = []\n",
    "    y = []\n",
    "    while True:\n",
    "        \n",
    "        # Check if we have a full batch and yield if we do\n",
    "        if len(x) == BATCH_SIZE:\n",
    "            x_np = np.array(x)\n",
    "            y_np = np.array(y)\n",
    "            print('-- x : ' + str(len(x_np)) + ' samples   shape: ' + str(x_np.shape))\n",
    "            yield x_np, y_np\n",
    "            x = []\n",
    "            y = []\n",
    "            BATCH_NSTEPS = BATCH_NSTEPS+1\n",
    "            if BATCH_NSTEPS > 23: BATCH_NSTEPS=7\n",
    "        \n",
    "        # Add next sample with BATCH_NSTEPS to training lists\n",
    "        istart = BATCH_STEP_INDEX[BATCH_NSTEPS]\n",
    "        for i in range(istart, istart+len(x_samples)):\n",
    "            idx = i%len(x_samples)\n",
    "            if len(x_samples[idx]) == BATCH_NSTEPS:\n",
    "                x.append(x_samples[idx])\n",
    "                y.append(y_samples[idx])\n",
    "                BATCH_STEP_INDEX[BATCH_NSTEPS] = idx+1\n",
    "                break\n",
    "\n",
    "train_generator = my_generator(x_train, y_train)\n",
    "valid_generator = my_generator(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "my_generator: 90009 samplesmy_generator: 10002 samples\n",
      "\n",
      "-- x : 100 samples   shape: (100, 7, 7)\n",
      "-- x : 100 samples   shape: (100, 7, 7)\n",
      "-- x : 100 samples   shape: (100, 8, 7)\n",
      "-- x : 100 samples   shape: (100, 8, 7)\n",
      "-- x : 100 samples   shape: (100, 9, 7)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking target: expected time_distributed_3 to have 3 dimensions, but got array with shape (100, 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-74fe6435874d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m   \u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m              \u001b[0;34m=\u001b[0m \u001b[0mEPOCHS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m   \u001b[0;34m,\u001b[0m\u001b[0minitial_epoch\u001b[0m       \u001b[0;34m=\u001b[0m \u001b[0mepoch_loaded\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m   \u001b[0;34m,\u001b[0m\u001b[0muse_multiprocessing\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m   \u001b[0;31m#,callbacks=[tensorboard, cbk]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m )\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1730\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1731\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1732\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1733\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    218\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                                             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m                                             reset_metrics=False)\n\u001b[0m\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[1;32m   1506\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1507\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1508\u001b[0;31m             class_weight=class_weight)\n\u001b[0m\u001b[1;32m   1509\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_uses_dynamic_learning_phase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    619\u001b[0m                 \u001b[0mfeed_output_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    620\u001b[0m                 \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 621\u001b[0;31m                 exception_prefix='target')\n\u001b[0m\u001b[1;32m    622\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    623\u001b[0m             \u001b[0;31m# Generate sample-wise weight values given the `sample_weight` and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    133\u001b[0m                         \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m                         \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' dimensions, but got array '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m                         'with shape ' + str(data_shape))\n\u001b[0m\u001b[1;32m    136\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m                     \u001b[0mdata_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking target: expected time_distributed_3 to have 3 dimensions, but got array with shape (100, 5)"
     ]
    }
   ],
   "source": [
    "\n",
    "EPOCHS = 10\n",
    "STEP_SIZE_TRAIN = len(x_train)/(23.-7.)/BATCH_SIZE\n",
    "STEP_SIZE_VALID = len(x_test)/(23.-7.)/BATCH_SIZE\n",
    "\n",
    "epoch_loaded = 0\n",
    "\n",
    "#model.fit_generator(train_generator(), steps_per_epoch=30, epochs=10, verbose=1)\n",
    "\n",
    "history = model.fit_generator(\n",
    "  generator            = train_generator\n",
    "  ,steps_per_epoch     = STEP_SIZE_TRAIN\n",
    "  ,validation_data     = valid_generator\n",
    "  ,validation_steps    = STEP_SIZE_VALID\n",
    "  ,epochs              = EPOCHS\n",
    "  ,initial_epoch       = epoch_loaded\n",
    "  ,use_multiprocessing = False\n",
    "  #,callbacks=[tensorboard, cbk]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
