{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JLab ML Challenge 2\n",
    "\n",
    "This notebook is an attempt at JLab ML Challenge 2. This was actually done some time after the challenge ended. My goal though is to take what was learned from the winning entries and implement it so as to better understand RNNs. This is the first step in applying this technology to actual data. \n",
    "\n",
    "In this section I recreate Andru Quiroga's model from his winning entry, but modify it a bit. Andru used an RNN, but fixed the number of time steps to be 7. In other words, he always used the last 7 detectors hit and ignored any other detectors when making his prediction. For the model defined below, I leave the number of time steps as undefined so that any number of them can be provided to the model and all will be used to predict the next step. This is done by specifying the input_shape of the first LSTM layer to have a shape of *(None,7)* instead of *(7,7)*. Note that the second *7* in the input_shape is for the input features. In this case, the 6 parameter state vector (*x,y,z,px,py,pz*) plus the z coordinate of the next detector plane that we are projecting to.\n",
    "\n",
    "The other change relative to Andru's model is that the last layer uses *TimeDistributed* instead of a single Dense layer. What this does is output a separate *Dense* layer for each time step. This allows us to obtain the predicted state vector for every time step (i.e. detector plane). This is very useful in training since it allows every timestep to contribute to the training. This point is worth expanding on a little. What this means is that I can train on only 24 hit tracks, but the weights will be adjusted to try and fit every plane, not just the last one. The final model can then be fed any number of detector planes (e.g. 14) and it will give a prediction for the 15th plane.\n",
    "\n",
    "In order to support the *TimeDistributed* layer, the **return_sequences** parameter of the last LSTM layer needed to be set to true. In Andru's model this was set to false and only the last time step passed to the final *Dense* layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, None, 500)         1016000   \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, None, 500)         2002000   \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, None, 500)         2002000   \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, None, 5)           2505      \n",
      "=================================================================\n",
      "Total params: 5,022,505\n",
      "Trainable params: 5,022,505\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import gzip\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Dense, TimeDistributed, Input, Lambda, LSTM\n",
    "\n",
    "model = Sequential()\n",
    "model.add( LSTM(units=500, return_sequences=True, input_shape=(None,7)) )  #  None -> undfined number of time steps\n",
    "model.add( LSTM(units=500, return_sequences=True) )\n",
    "model.add( LSTM(units=500, return_sequences=True) )\n",
    "model.add( TimeDistributed(Dense(units=5)) )\n",
    "\n",
    "model.compile(optimizer='adam', loss='mean_squared_error', metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in training data\n",
    "\n",
    "In this section I read in the training data. The training file is in csv format with 150 values per line. These correspond to the 25 state vectors, each with 6 parameters. The first state vector is at the target position while all others are at detector planes. For this challenge, the target position is considered a detector plane itself. For detector plane (except the last) a 7-parameter feature list is created using the 6 parameter state vector plus the z corrdinate of the next detector plane. For the labels, we use the 5 parameters of the state vector (z coordinate is excluded) at the next plane. Thus, there are 24 *time steps* with 24 *labels* for each row of the input file.\n",
    "\n",
    "Note that not every sample in the input file is used here since I was having problems with the server jupyter process dying and suspected it was due to memory issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading input data ...\n",
      "Training samples: 45000 (45000)\n",
      " Testing samples: 5000 (5000)\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "print('Reading input data ...')\n",
    "\n",
    "with open('MLchallenge2_training.csv') as csv_file:\n",
    "    csv_file.readline() # discard header line\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    \n",
    "    x_all = []\n",
    "    y_all = []\n",
    "    for row in csv_reader:\n",
    "        \n",
    "        # Copy each state vector (6 values) into individual list with a 7th value being\n",
    "        # the z of the next detector plane. At the same time, create 5 parameter label.\n",
    "        features = []\n",
    "        labels = []\n",
    "        for i in range(0, 24):\n",
    "            idx = i*6\n",
    "            features.append(row[idx:idx+6]+[row[idx+6+2]])\n",
    "            idx += 6\n",
    "            labels.append(row[idx:idx+2]+row[idx+3:idx+6])\n",
    "        x_all.append(features)\n",
    "        y_all.append(labels)\n",
    "        \n",
    "        # Limit how many samples we use\n",
    "        if len(y_all) >=50000: break\n",
    "\n",
    "TRAIN_FRACTION = 0.90\n",
    "idx = int(len(x_all)*TRAIN_FRACTION)\n",
    "x_train = np.array(x_all[0:idx])\n",
    "y_train = np.array(y_all[0:idx])\n",
    "x_test  = np.array(x_all[idx:])\n",
    "y_test  = np.array(y_all[idx:])\n",
    "\n",
    "# Not sure if this allows memory to be freed, but maybe ...\n",
    "x_all = []\n",
    "y_all = []\n",
    "\n",
    "print('Training samples: ' + str(len(x_train)) + ' (' + str(len(y_train)) +')')\n",
    "print(' Testing samples: ' + str(len(x_test )) + ' (' + str(len(y_test )) +')')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data generator\n",
    "\n",
    "In this section I set up the data generator. This is really not the right way to do this. I should either have combined reading the file with the generator so that I didn't have to limit the samples in the previous section, *OR* I should not use a generator at all and just pass the data into Keras as arrays. This is basically an artifact from my first attempt at this when the generator was much more complicated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 10\n",
    "\n",
    "def my_generator(x_samples, y_samples):\n",
    "    global BATCH_SIZE\n",
    "\n",
    "    BATCH_NSTEPS = 7\n",
    "    BATCH_INDEX  = 0\n",
    "    NSAMPLES = len(x_samples)\n",
    "\n",
    "    print('my_generator: ' + str(len(x_samples)) + ' samples')\n",
    "    \n",
    "    while True:\n",
    "        x = []\n",
    "        y = []\n",
    "        for i in range(0, BATCH_SIZE):\n",
    "            x.append( x_samples[BATCH_INDEX] )\n",
    "            y.append( y_samples[BATCH_INDEX] )\n",
    "            BATCH_INDEX = (BATCH_INDEX+1)%NSAMPLES\n",
    "        \n",
    "        x_np = np.array(x)\n",
    "        y_np = np.array(y)\n",
    "        yield x_np, y_np\n",
    "\n",
    "train_generator = my_generator(x_train, y_train)\n",
    "valid_generator = my_generator(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit the model\n",
    "\n",
    "***BE CAREFUL RUNNING THIS CELL!***\n",
    "\n",
    "In this section we fit the model. Using Jupyter to do this is actually really slow due to the hardware allocated to the notebook. Running this on one of the sciml190X machines with multiple Titan RX GPUs sped this up by a factor of about 10-20. Because I don't want this to accidentally overwrite the model file, I only have it fit and save the model if *FIT_MODEL* is set to True. I leave it set to False so I can run all of the cells here without overwriting the model file.\n",
    "\n",
    "To run this on one of the sciml190X machines, I exported this notebook as an executable script. Here are the details for reproduceability. Pleae note that these are based on magic that Will Phelps figured out and shared with me:\n",
    "\n",
    "1. **File->Export Notebook As...->Export Notebook To Executable Script**<br>\n",
    "Transfer the produced script (*2019.12.31.MLChallenge2.py*) to the working directory on the CUE. In my case: */home/davidl/Jupyter/2019.12.31.MLChallenge2*\n",
    "\n",
    "2. **Setup conda environment. Only do this once. Skip to section 3 if you've already done this:**<br>\n",
    "2a. ssh ifarm1802<br>\n",
    "2b. ln -s /work/halld2/home/davidl/builds/CONDA_ENV ~/.conda<br>\n",
    "2c. bash<br>\n",
    "2d. source /etc/profile.d/modules.sh<br>\n",
    "2e. module use /apps/modulefiles<br>\n",
    "2f. module load anaconda2/4.5.12<br>\n",
    "2g. conda create -n tf-gpu tensorflow-gpu=1.14 cudatoolkit=10.0 keras numpy=1.16.4<br>\n",
    "2h. conda activate tf-gpu<br>\n",
    "2i. pip install Pillow<br>\n",
    "2j. conda install ipython matplotlib<br>\n",
    "\n",
    "3. **Allocate interactive session on one of the scml190X nodes**<br>\n",
    "3a. salloc --gres gpu:TitanRTX:2 --partition gpu --nodes 1<br>\n",
    "3b. srun --pty bash<br>\n",
    "3c. source /etc/profile.d/modules.sh<br>\n",
    "3d. module use /apps/modulefiles<br>\n",
    "3e. module load anaconda2/4.5.12<br>\n",
    "3f. conda activate tf-gpu<br>\n",
    "\n",
    "4. **Go to working directory and run script**<br>\n",
    "4a. cd /home/davidl/Jupyter/2019.12.31.MLChallenge2<br>\n",
    "4b. ipython3 2019.12.31.MLChallenge2.py<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "FIT_MODEL = False\n",
    "EPOCHS = 20\n",
    "STEP_SIZE_TRAIN = len(x_train)/BATCH_SIZE\n",
    "STEP_SIZE_VALID = len(x_test)/BATCH_SIZE\n",
    "model_fname = 'MyModel.h5'\n",
    "\n",
    "if FIT_MODEL:\n",
    "    history = model.fit_generator(\n",
    "      generator            = train_generator\n",
    "      ,steps_per_epoch     = STEP_SIZE_TRAIN\n",
    "      ,validation_data     = valid_generator\n",
    "      ,validation_steps    = STEP_SIZE_VALID\n",
    "      ,epochs              = EPOCHS\n",
    "      ,use_multiprocessing = False\n",
    "    )\n",
    "\n",
    "    model.save(model_fname)\n",
    "    print('Model saved to: ' + model_fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the model using the Test data set\n",
    "\n",
    "In this section I load the model and feed it some tracks from the Test data set. In the Test set, each row of the inputs file will have a diffrent number of entries depending on how many detector planes are present. This will format the inputs into the 7 parameter features the model expects with as many time steps as detector planes.\n",
    "\n",
    "I then loop through the outputs file which contains 5 parameters per row corresponding to the expected output from the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: MyModel.h5\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, None, 500)         1016000   \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, None, 500)         2002000   \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, None, 500)         2002000   \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, None, 5)           2505      \n",
      "=================================================================\n",
      "Total params: 5,022,505\n",
      "Trainable params: 5,022,505\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      " Number tracks: 1000\n",
      "    Total time: 56.228 sec\n",
      "Time per track: 56.228 msec\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import time\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "print('Loading model: ' + model_fname)\n",
    "model = load_model(model_fname)\n",
    "model.summary()\n",
    "\n",
    "inputs_fname = 'MLchallenge2_testing_inputs.csv'\n",
    "outputs_fname = 'MLchallenge2_testing_outputs.csv'\n",
    "\n",
    "# Read in all inputs\n",
    "with open(inputs_fname) as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    \n",
    "    x = []\n",
    "    for row in csv_reader:\n",
    "        \n",
    "        # Copy each state vector (6 values) into individual list with a 7th value being\n",
    "        # the z of the next detector plane. At the same time, create 5 parameter label.\n",
    "        features = []\n",
    "        Ndets = int((len(row)-1)/6)\n",
    "        for i in range(0, Ndets):\n",
    "            idx = i*6\n",
    "            idx_next_z = idx+6+2\n",
    "            if idx_next_z >= len(row): idx_next_z = len(row)-1\n",
    "            features.append(row[idx:idx+6]+[row[idx_next_z]])\n",
    "        x.append(features)\n",
    "\n",
    "        # Limit how many samples we use\n",
    "        if len(x) >=1000: break\n",
    "\n",
    "# Read in all output targets\n",
    "with open(outputs_fname) as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    \n",
    "    y = []\n",
    "    for row in csv_reader:\n",
    "        y.append(row)\n",
    "\n",
    "        # Limit how many samples we use\n",
    "        if len(y) >=len(x): break\n",
    "\n",
    "tstart = time.time()\n",
    "for i in range(0, len(x)):\n",
    "    model.reset_states()\n",
    "    pred = model.predict([[x[i]]])\n",
    "\n",
    "tend = time.time()\n",
    "tdiff = tend - tstart\n",
    "\n",
    "print(' Number tracks: %d' % len(x))\n",
    "print('    Total time: %3.3f sec' % tdiff)\n",
    "print('Time per track: %3.3f msec' % (tdiff*1000.0/len(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
